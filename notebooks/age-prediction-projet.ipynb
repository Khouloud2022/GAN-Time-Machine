{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":78156,"sourceType":"datasetVersion","datasetId":44109}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch torchvision timm albumentations scikit-learn matplotlib\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T17:37:36.789630Z","iopub.execute_input":"2025-12-09T17:37:36.789855Z","iopub.status.idle":"2025-12-09T17:39:05.756906Z","shell.execute_reply.started":"2025-12-09T17:37:36.789834Z","shell.execute_reply":"2025-12-09T17:39:05.756012Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.19)\nRequirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.8)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.3)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.36.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\nRequirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.15.3)\nRequirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.12.4)\nRequirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.24)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.12.0.88)\nRequirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (3.12.5)\nRequirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (6.5.0)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nCollecting numpy (from torchvision)\n  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.5)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.10.5)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nINFO: pip is looking at multiple versions of mkl-fft to determine which version is compatible with other requirements. This could take a while.\nCollecting mkl_fft (from numpy->torchvision)\n  Downloading mkl_fft-2.1.1-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.3 kB)\n  Downloading mkl_fft-2.0.0-22-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.1 kB)\nINFO: pip is looking at multiple versions of mkl-random to determine which version is compatible with other requirements. This could take a while.\nCollecting mkl_random (from numpy->torchvision)\n  Downloading mkl_random-1.3.0-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n  Downloading mkl_random-1.2.11-22-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.1 kB)\nINFO: pip is looking at multiple versions of mkl-umath to determine which version is compatible with other requirements. This could take a while.\nCollecting mkl_umath (from numpy->torchvision)\n  Downloading mkl_umath-0.3.0-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\n  Downloading mkl_umath-0.2.0-21-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.2 kB)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-2.2.6 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade numpy scikit-learn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T17:39:05.758559Z","iopub.execute_input":"2025-12-09T17:39:05.758848Z","iopub.status.idle":"2025-12-09T17:39:20.509653Z","shell.execute_reply.started":"2025-12-09T17:39:05.758817Z","shell.execute_reply":"2025-12-09T17:39:20.508667Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.2.6)\nCollecting numpy\n  Downloading numpy-2.3.5-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m57.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nCollecting scikit-learn\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nDownloading numpy-2.3.5-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy, scikit-learn\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.2.6\n    Uninstalling numpy-2.2.6:\n      Successfully uninstalled numpy-2.2.6\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.5 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.5 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.5 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.5 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-2.3.5 scikit-learn-1.7.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -U albumentations\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T17:39:20.510758Z","iopub.execute_input":"2025-12-09T17:39:20.511016Z","iopub.status.idle":"2025-12-09T17:39:27.553182Z","shell.execute_reply.started":"2025-12-09T17:39:20.510990Z","shell.execute_reply":"2025-12-09T17:39:27.552333Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.8)\nRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.3.5)\nRequirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.15.3)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.3)\nRequirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.12.4)\nRequirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.24)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.12.0.88)\nRequirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (3.12.5)\nRequirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (6.5.0)\nCollecting numpy>=1.24.4 (from albumentations)\n  Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.41.5)\nRequirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (4.15.0)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.2)\nUsing cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\nInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.3.5\n    Uninstalling numpy-2.3.5:\n      Successfully uninstalled numpy-2.3.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-2.2.6\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nos.kill(os.getpid(), 9)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-09T17:39:27.021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# Imports\n# -----------------------------\nimport os, glob\nfrom PIL import Image\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport torch\nimport torch.nn as nn\nimport timm\nfrom tqdm import tqdm\nimport random\n\n# -----------------------------\n# Dataset\n# -----------------------------\nclass UTKFaceDataset(Dataset):\n    def __init__(self, image_paths, transform=None):\n        self.image_paths = image_paths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        img = Image.open(img_path).convert(\"RGB\")\n        age = int(os.path.basename(img_path).split('_')[0])\n\n        img = np.array(img)\n        if self.transform:\n            img = self.transform(image=img)['image']\n\n        return img, age\n\n\ntransform_train = A.Compose([\n    A.RandomResizedCrop(\n        height=224, \n        width=224, \n        size=(224, 224),   # OBLIGATOIRE sur Kaggle\n        scale=(0.85, 1.0)\n    ),\n    A.HorizontalFlip(p=0.5),\n    A.ShiftScaleRotate(\n        shift_limit=0.03,\n        scale_limit=0.08,\n        rotate_limit=10,\n        p=0.5\n    ),\n    A.RandomBrightnessContrast(p=0.35),\n    A.CLAHE(p=0.2),\n    A.GaussianBlur(blur_limit=(3,5), p=0.2),\n    A.CoarseDropout(max_holes=1, max_height=32, max_width=32, p=0.25),\n    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n    ToTensorV2()\n])\n\n\ntransform_test = A.Compose([\n    A.Resize(height=224, width=224),\n    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n    ToTensorV2()\n])\n\n\n# -----------------------------\n# Split\n# -----------------------------\nimage_paths = glob.glob(\"/kaggle/input/utkface-new/UTKFace/*jpg\")\n\ntrain_val_paths, test_paths = train_test_split(image_paths, test_size=0.2, random_state=42)\ntrain_paths, val_paths      = train_test_split(train_val_paths, test_size=0.2, random_state=42)\n\ntrain_dataset = UTKFaceDataset(train_paths, transform=transform_train)\nval_dataset   = UTKFaceDataset(val_paths, transform=transform_test)\ntest_dataset  = UTKFaceDataset(test_paths, transform=transform_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\ntest_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n\n\n# -----------------------------\n# Fonction MixUp légère\n# -----------------------------\ndef mixup(imgs, ages, alpha=0.2):\n    if alpha <= 0:\n        return imgs, ages\n\n    lam = np.random.beta(alpha, alpha)\n    batch_size = imgs.size(0)\n    index = torch.randperm(batch_size).to(imgs.device)\n\n    mixed_imgs = lam * imgs + (1 - lam) * imgs[index]\n    mixed_ages = lam * ages + (1 - lam) * ages[index]\n\n    return mixed_imgs, mixed_ages\n\n\n# -----------------------------\n# Modèle ViT\n# -----------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = timm.create_model('vit_base_patch16_224', pretrained=True)\n\n# Freeze total\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze dernier 1/3\nfor param in model.blocks[8:].parameters():\n    param.requires_grad = True\n\n# Tête\nin_features = model.head.in_features\nmodel.head = nn.Sequential(\n    nn.Dropout(0.4),\n    nn.Linear(in_features, 1)\n)\nmodel = model.to(device)\n\n# -----------------------------\n# Training setup\n# -----------------------------\ncriterion = nn.L1Loss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=2, verbose=True\n)\n\nepochs = 20\nbest_val_loss = float('inf')\npatience = 5\npatience_counter = 0\n\n# -----------------------------\n# Training Loop\n# -----------------------------\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0\n\n    for imgs, ages in tqdm(train_loader):\n        imgs = imgs.to(device)\n        ages = ages.float().unsqueeze(1).to(device)\n\n        # MixUp (light)\n        imgs, ages = mixup(imgs, ages, alpha=0.15)\n\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, ages)\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n\n        running_loss += loss.item() * imgs.size(0)\n\n    train_loss = running_loss / len(train_loader.dataset)\n\n    # Validation\n    model.eval()\n    val_loss = 0\n    val_mae = 0\n\n    with torch.no_grad():\n        for imgs, ages in val_loader:\n            imgs = imgs.to(device)\n            ages = ages.float().unsqueeze(1).to(device)\n            outputs = model(imgs)\n\n            loss = criterion(outputs, ages)\n            val_loss += loss.item() * imgs.size(0)\n            val_mae += torch.abs(outputs - ages).sum().item()\n\n    val_loss /= len(val_loader.dataset)\n    val_mae  /= len(val_loader.dataset)\n\n    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val MAE: {val_mae:.2f}\")\n\n    scheduler.step(val_loss)\n\n    # Early stopping\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), \"/kaggle/working/best_vit_utkface.pth\")\n        print(\"✔️ Nouveau meilleur modèle sauvegardé\")\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(\"⛔ Early stopping\")\n            break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:03:04.994736Z","iopub.execute_input":"2025-12-09T18:03:04.995400Z","iopub.status.idle":"2025-12-09T19:56:21.432744Z","shell.execute_reply.started":"2025-12-09T18:03:04.995357Z","shell.execute_reply":"2025-12-09T19:56:21.431572Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_116/3924852830.py:41: UserWarning: Argument(s) 'height, width' are not valid for transform RandomResizedCrop\n  A.RandomResizedCrop(\n/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n  original_init(self, **validated_kwargs)\n/tmp/ipykernel_116/3924852830.py:57: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n  A.CoarseDropout(max_holes=1, max_height=32, max_width=32, p=0.25),\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06ed4ce1db0f45d99e16c31fd875c4ff"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n100%|██████████| 475/475 [04:40<00:00,  1.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/20\nTrain Loss: 8.0544 | Val Loss: 6.2705 | Val MAE: 6.27\n✔️ Nouveau meilleur modèle sauvegardé\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:54<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/20\nTrain Loss: 6.1698 | Val Loss: 5.7224 | Val MAE: 5.72\n✔️ Nouveau meilleur modèle sauvegardé\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:54<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/20\nTrain Loss: 5.7585 | Val Loss: 6.0308 | Val MAE: 6.03\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:54<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/20\nTrain Loss: 5.4931 | Val Loss: 5.9964 | Val MAE: 6.00\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:55<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/20\nTrain Loss: 5.2910 | Val Loss: 5.3594 | Val MAE: 5.36\n✔️ Nouveau meilleur modèle sauvegardé\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:54<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/20\nTrain Loss: 5.0891 | Val Loss: 5.1476 | Val MAE: 5.15\n✔️ Nouveau meilleur modèle sauvegardé\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:55<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/20\nTrain Loss: 4.9143 | Val Loss: 4.9711 | Val MAE: 4.97\n✔️ Nouveau meilleur modèle sauvegardé\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:53<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/20\nTrain Loss: 4.7500 | Val Loss: 5.0962 | Val MAE: 5.10\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:54<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/20\nTrain Loss: 4.6994 | Val Loss: 4.9826 | Val MAE: 4.98\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:54<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/20\nTrain Loss: 4.5945 | Val Loss: 4.9973 | Val MAE: 5.00\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:53<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11/20\nTrain Loss: 4.3488 | Val Loss: 4.9106 | Val MAE: 4.91\n✔️ Nouveau meilleur modèle sauvegardé\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:53<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12/20\nTrain Loss: 4.1283 | Val Loss: 5.0771 | Val MAE: 5.08\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:54<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13/20\nTrain Loss: 4.0718 | Val Loss: 4.9566 | Val MAE: 4.96\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:54<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14/20\nTrain Loss: 4.0016 | Val Loss: 4.8406 | Val MAE: 4.84\n✔️ Nouveau meilleur modèle sauvegardé\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:54<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 15/20\nTrain Loss: 4.0308 | Val Loss: 4.8796 | Val MAE: 4.88\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:54<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 16/20\nTrain Loss: 3.8892 | Val Loss: 4.8999 | Val MAE: 4.90\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:54<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 17/20\nTrain Loss: 3.8262 | Val Loss: 4.8572 | Val MAE: 4.86\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:55<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 18/20\nTrain Loss: 3.7580 | Val Loss: 4.8510 | Val MAE: 4.85\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:54<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 19/20\nTrain Loss: 3.7012 | Val Loss: 4.8065 | Val MAE: 4.81\n✔️ Nouveau meilleur modèle sauvegardé\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:54<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 20/20\nTrain Loss: 3.6685 | Val Loss: 4.8328 | Val MAE: 4.83\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# -----------------------------\n# Imports\n# -----------------------------\nimport os, glob\nfrom PIL import Image\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport torch\nimport torch.nn as nn\nimport timm\nfrom tqdm import tqdm\n\n# -----------------------------\n# Dataset\n# -----------------------------\nclass UTKFaceDataset(Dataset):\n    def __init__(self, image_paths, transform=None):\n        self.image_paths = image_paths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        img = Image.open(img_path).convert(\"RGB\")\n        age = int(os.path.basename(img_path).split('_')[0])\n        img = np.array(img)\n        if self.transform:\n            img = self.transform(image=img)['image']\n        return img, age\n\n# -----------------------------\n# Augmentations\n# -----------------------------\ntransform_train = A.Compose([\n    A.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n    A.HorizontalFlip(p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5),\n    A.RandomBrightnessContrast(p=0.3),\n    A.CoarseDropout(max_holes=1, max_height=32, max_width=32, p=0.3),\n    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n    ToTensorV2()\n])\n\n\ntransform_test = A.Compose([\n    A.Resize(224, 224),\n    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n    ToTensorV2()\n])\n\n# -----------------------------\n# Chargement des images avec train/val/test split\n# -----------------------------\nimage_paths = glob.glob(\"/kaggle/input/utkface-new/UTKFace/*jpg\")\n\n# 20% test\ntrain_val_paths, test_paths = train_test_split(image_paths, test_size=0.2, random_state=42)\n\n# 80% train, 20% validation (sur le train_val)\ntrain_paths, val_paths = train_test_split(train_val_paths, test_size=0.2, random_state=42)\n\n# Création des datasets\ntrain_dataset = UTKFaceDataset(train_paths, transform=transform_train)\nval_dataset   = UTKFaceDataset(val_paths, transform=transform_test)\ntest_dataset  = UTKFaceDataset(test_paths, transform=transform_test)\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\ntest_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n\n# -----------------------------\n# Modèle ViT\n# -----------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = timm.create_model('vit_base_patch16_224', pretrained=True)\n\n# Freeze partiel pour éviter l'overfitting\nfor param in model.parameters():\n    param.requires_grad = False\n\nfor param in model.blocks[8:].parameters():  # dégel des derniers blocs\n    param.requires_grad = True\nfor param in model.head.parameters():\n    param.requires_grad = True\n\n# Tête avec Dropout\nin_features = model.head.in_features\nmodel.head = nn.Sequential(\n    nn.Dropout(0.5),\n    nn.Linear(in_features, 1)\n)\nmodel = model.to(device)\n\n# -----------------------------\n# Perte, optimiseur et scheduler\n# -----------------------------\ncriterion = nn.L1Loss()  # moins sensible aux outliers\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T19:59:16.908099Z","iopub.execute_input":"2025-12-09T19:59:16.908460Z","iopub.status.idle":"2025-12-09T19:59:18.809675Z","shell.execute_reply.started":"2025-12-09T19:59:16.908426Z","shell.execute_reply":"2025-12-09T19:59:18.808886Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_116/2196365205.py:44: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n  A.CoarseDropout(max_holes=1, max_height=32, max_width=32, p=0.3),\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# -----------------------------\n# Entraînement avec early stopping\n# -----------------------------\nepochs = 20\nbest_val_loss = float('inf')\npatience = 5  # patience plus longue pour éviter arrêt prématuré\ncounter = 0\n\nfor epoch in range(epochs):\n    # Training\n    model.train()\n    running_loss = 0\n    for imgs, ages in tqdm(train_loader):\n        imgs, ages = imgs.to(device), ages.float().unsqueeze(1).to(device)\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, ages)\n        loss.backward()\n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        running_loss += loss.item() * imgs.size(0)\n    train_loss = running_loss / len(train_loader.dataset)\n\n    # Validation\n    model.eval()\n    val_loss = 0\n    val_mae = 0\n    with torch.no_grad():\n        for imgs, ages in val_loader:\n            imgs, ages = imgs.to(device), ages.float().unsqueeze(1).to(device)\n            outputs = model(imgs)\n            loss = criterion(outputs, ages)\n            val_loss += loss.item() * imgs.size(0)\n            val_mae += torch.abs(outputs - ages).sum().item()\n    val_loss /= len(test_loader.dataset)\n    val_mae /= len(test_loader.dataset)\n\n    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f} - Val MAE: {val_mae:.4f}\")\n\n    # Scheduler\n    scheduler.step(val_loss)\n\n    # Early stopping + sauvegarde\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        counter = 0\n        torch.save(model.state_dict(), \"/kaggle/working/best_vit_utkface.pth\")\n        print(\"✅ Nouveau meilleur modèle sauvegardé\")\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"⏹ Early stopping déclenché\")\n            break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T19:59:22.838662Z","iopub.execute_input":"2025-12-09T19:59:22.838966Z","iopub.status.idle":"2025-12-09T21:52:34.222116Z","shell.execute_reply.started":"2025-12-09T19:59:22.838941Z","shell.execute_reply":"2025-12-09T21:52:34.221081Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 475/475 [04:54<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20 - Train Loss: 8.4301 - Val Loss: 4.9656 - Val MAE: 4.9656\n✅ Nouveau meilleur modèle sauvegardé\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:54<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/20 - Train Loss: 6.1679 - Val Loss: 4.3972 - Val MAE: 4.3972\n✅ Nouveau meilleur modèle sauvegardé\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:54<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/20 - Train Loss: 5.5857 - Val Loss: 4.2587 - Val MAE: 4.2587\n✅ Nouveau meilleur modèle sauvegardé\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:53<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/20 - Train Loss: 5.2258 - Val Loss: 4.3142 - Val MAE: 4.3142\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:53<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/20 - Train Loss: 4.9817 - Val Loss: 4.1614 - Val MAE: 4.1614\n✅ Nouveau meilleur modèle sauvegardé\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:53<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/20 - Train Loss: 4.7480 - Val Loss: 4.1077 - Val MAE: 4.1077\n✅ Nouveau meilleur modèle sauvegardé\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:54<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/20 - Train Loss: 4.4825 - Val Loss: 4.2513 - Val MAE: 4.2513\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:53<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/20 - Train Loss: 4.3751 - Val Loss: 4.0326 - Val MAE: 4.0326\n✅ Nouveau meilleur modèle sauvegardé\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:53<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/20 - Train Loss: 4.1624 - Val Loss: 4.0594 - Val MAE: 4.0594\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:53<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/20 - Train Loss: 4.0295 - Val Loss: 3.9758 - Val MAE: 3.9758\n✅ Nouveau meilleur modèle sauvegardé\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:53<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/20 - Train Loss: 3.9177 - Val Loss: 4.0698 - Val MAE: 4.0698\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:53<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/20 - Train Loss: 3.7705 - Val Loss: 3.9156 - Val MAE: 3.9156\n✅ Nouveau meilleur modèle sauvegardé\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:54<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/20 - Train Loss: 3.6354 - Val Loss: 4.0232 - Val MAE: 4.0232\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:54<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/20 - Train Loss: 3.5293 - Val Loss: 3.9833 - Val MAE: 3.9833\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:53<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/20 - Train Loss: 3.4415 - Val Loss: 4.2560 - Val MAE: 4.2560\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:54<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/20 - Train Loss: 3.1875 - Val Loss: 3.8416 - Val MAE: 3.8416\n✅ Nouveau meilleur modèle sauvegardé\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:53<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/20 - Train Loss: 3.0566 - Val Loss: 3.9799 - Val MAE: 3.9799\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:52<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/20 - Train Loss: 2.9383 - Val Loss: 3.8875 - Val MAE: 3.8875\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:53<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/20 - Train Loss: 2.9159 - Val Loss: 3.8478 - Val MAE: 3.8478\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 475/475 [04:54<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20/20 - Train Loss: 2.7337 - Val Loss: 3.8012 - Val MAE: 3.8012\n✅ Nouveau meilleur modèle sauvegardé\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"\n# -----------------------------\n# 8️⃣ Évaluation MAE\n# -----------------------------\nmodel.load_state_dict(torch.load(\"/kaggle/working/best_vit_utkface.pth\"))\nmodel.eval()\n\nages_true = []\nages_pred = []\n\nwith torch.no_grad():\n    for imgs, ages in test_loader:\n        imgs = imgs.to(device)\n        outputs = model(imgs)\n        ages_pred.extend(outputs.cpu().numpy())\n        ages_true.extend(ages.numpy())\n\nages_pred = np.array(ages_pred).flatten()\nages_true = np.array(ages_true).flatten()\nmae = np.mean(np.abs(ages_true - ages_pred))\nprint(f\"📊 Mean Absolute Error (MAE) sur le test set : {mae:.2f} ans\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T21:53:13.766515Z","iopub.execute_input":"2025-12-09T21:53:13.766838Z","iopub.status.idle":"2025-12-09T21:54:12.054324Z","shell.execute_reply.started":"2025-12-09T21:53:13.766808Z","shell.execute_reply":"2025-12-09T21:54:12.053386Z"}},"outputs":[{"name":"stdout","text":"📊 Mean Absolute Error (MAE) sur le test set : 4.60 ans\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Sauvegarder uniquement les poids du modèle\ntorch.save(model.state_dict(), \"/kaggle/working/vit_utkface.pth\")\n\n# Ou sauvegarder le modèle complet (architecture + poids)\ntorch.save(model, \"/kaggle/working/vit_utkface_full.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T21:54:22.772301Z","iopub.execute_input":"2025-12-09T21:54:22.773141Z","iopub.status.idle":"2025-12-09T21:54:23.536195Z","shell.execute_reply.started":"2025-12-09T21:54:22.773105Z","shell.execute_reply":"2025-12-09T21:54:23.535453Z"}},"outputs":[],"execution_count":11}]}