{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10610809,"sourceType":"datasetVersion","datasetId":6552979}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install insightface --upgrade --quiet\n!pip install onnxruntime-gpu --upgrade --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T09:06:10.302677Z","iopub.execute_input":"2025-12-09T09:06:10.302909Z","iopub.status.idle":"2025-12-09T09:06:47.355034Z","shell.execute_reply.started":"2025-12-09T09:06:10.302885Z","shell.execute_reply":"2025-12-09T09:06:47.354120Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m439.5/439.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Building wheel for insightface (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m300.5/300.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\n# VÃ©rifier la version du driver NVIDIA et du GPU\n!nvidia-smi\n\n# VÃ©rifier la version de CUDA installÃ©e\n!nvcc --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T09:06:47.356835Z","iopub.execute_input":"2025-12-09T09:06:47.357203Z","iopub.status.idle":"2025-12-09T09:06:47.708566Z","shell.execute_reply.started":"2025-12-09T09:06:47.357173Z","shell.execute_reply":"2025-12-09T09:06:47.707890Z"}},"outputs":[{"name":"stdout","text":"Tue Dec  9 09:06:47 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   33C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   35C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2024 NVIDIA Corporation\nBuilt on Thu_Jun__6_02:18:23_PDT_2024\nCuda compilation tools, release 12.5, V12.5.82\nBuild cuda_12.5.r12.5/compiler.34385749_0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# VÃ©rifier la version de cuDNN\n!cat /usr/include/cudnn_version.h | grep CUDNN_MAJOR -A 2\n!sudo ln -sf /usr/local/cuda/lib64/libcublasLt.so.12 /usr/local/cuda/lib64/libcublasLt.so.11\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T09:06:47.709443Z","iopub.execute_input":"2025-12-09T09:06:47.709673Z","iopub.status.idle":"2025-12-09T09:06:47.988510Z","shell.execute_reply.started":"2025-12-09T09:06:47.709649Z","shell.execute_reply":"2025-12-09T09:06:47.987573Z"}},"outputs":[{"name":"stdout","text":"#define CUDNN_MAJOR 9\n#define CUDNN_MINOR 2\n#define CUDNN_PATCHLEVEL 1\n--\n#define CUDNN_VERSION (CUDNN_MAJOR * 10000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\n\n/* cannot use constexpr here since this is a C-only file */\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"ls -l /usr/local/cuda/lib64/libcublasLt.so.11\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T09:06:47.989505Z","iopub.execute_input":"2025-12-09T09:06:47.989725Z","iopub.status.idle":"2025-12-09T09:06:48.107666Z","shell.execute_reply.started":"2025-12-09T09:06:47.989703Z","shell.execute_reply":"2025-12-09T09:06:48.106991Z"}},"outputs":[{"name":"stdout","text":"lrwxrwxrwx 1 root root 39 Dec  9 09:06 \u001b[0m\u001b[01;36m/usr/local/cuda/lib64/libcublasLt.so.11\u001b[0m -> /usr/local/cuda/lib64/libcublasLt.so.12\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install onnxruntime-gpu==1.20.0  # compatible CUDA 12\nimport onnxruntime as ort\nprint(\"Available providers:\", ort.get_available_providers())\n!pip install insightface onnxruntime-gpu tqdm\n!pip uninstall -y matplotlib insightface onnxruntime onnxruntime-gpu\n!pip install matplotlib==3.8.4 insightface onnxruntime-gpu tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T09:06:48.109587Z","iopub.execute_input":"2025-12-09T09:06:48.109792Z","iopub.status.idle":"2025-12-09T09:07:28.580699Z","shell.execute_reply.started":"2025-12-09T09:06:48.109772Z","shell.execute_reply":"2025-12-09T09:07:28.580006Z"}},"outputs":[{"name":"stdout","text":"Collecting onnxruntime-gpu==1.20.0\n  Downloading onnxruntime_gpu-1.20.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu==1.20.0) (15.0.1)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu==1.20.0) (25.2.10)\nRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu==1.20.0) (2.2.6)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu==1.20.0) (25.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu==1.20.0) (6.33.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu==1.20.0) (1.13.1)\nRequirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime-gpu==1.20.0) (10.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime-gpu==1.20.0) (1.3.0)\nDownloading onnxruntime_gpu-1.20.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (291.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m291.5/291.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: onnxruntime-gpu\n  Attempting uninstall: onnxruntime-gpu\n    Found existing installation: onnxruntime-gpu 1.23.2\n    Uninstalling onnxruntime-gpu-1.23.2:\n      Successfully uninstalled onnxruntime-gpu-1.23.2\nSuccessfully installed onnxruntime-gpu-1.20.0\nAvailable providers: ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\nRequirement already satisfied: insightface in /usr/local/lib/python3.11/dist-packages (0.7.3)\nRequirement already satisfied: onnxruntime-gpu in /usr/local/lib/python3.11/dist-packages (1.20.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from insightface) (2.2.6)\nRequirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from insightface) (1.18.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from insightface) (2.32.5)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from insightface) (3.7.2)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from insightface) (11.3.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from insightface) (1.15.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from insightface) (1.2.2)\nRequirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from insightface) (0.25.2)\nRequirement already satisfied: easydict in /usr/local/lib/python3.11/dist-packages (from insightface) (1.13)\nRequirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (from insightface) (3.0.12)\nRequirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (from insightface) (2.0.8)\nRequirement already satisfied: prettytable in /usr/local/lib/python3.11/dist-packages (from insightface) (3.16.0)\nRequirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (15.0.1)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (25.2.10)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (25.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (6.33.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (1.13.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations->insightface) (6.0.3)\nRequirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations->insightface) (2.12.4)\nRequirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations->insightface) (0.0.24)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations->insightface) (4.12.0.88)\nRequirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations->insightface) (3.12.5)\nRequirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations->insightface) (6.5.0)\nRequirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime-gpu) (10.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (1.4.8)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (2.9.0.post0)\nRequirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx->insightface) (4.15.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prettytable->insightface) (0.2.13)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->insightface) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->insightface) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->insightface) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->insightface) (2025.10.5)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (3.5)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (2.37.0)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (2025.6.11)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (0.4)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->insightface) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->insightface) (3.6.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations->insightface) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations->insightface) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations->insightface) (0.4.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->insightface) (1.17.0)\nFound existing installation: matplotlib 3.7.2\nUninstalling matplotlib-3.7.2:\n  Successfully uninstalled matplotlib-3.7.2\nFound existing installation: insightface 0.7.3\nUninstalling insightface-0.7.3:\n  Successfully uninstalled insightface-0.7.3\n\u001b[33mWARNING: Skipping onnxruntime as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mFound existing installation: onnxruntime-gpu 1.20.0\nUninstalling onnxruntime-gpu-1.20.0:\n  Successfully uninstalled onnxruntime-gpu-1.20.0\nCollecting matplotlib==3.8.4\n  Downloading matplotlib-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\nCollecting insightface\n  Using cached insightface-0.7.3-cp311-cp311-linux_x86_64.whl\nCollecting onnxruntime-gpu\n  Using cached onnxruntime_gpu-1.23.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4) (1.4.8)\nRequirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4) (2.2.6)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4) (25.0)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4) (11.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4) (2.9.0.post0)\nRequirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from insightface) (1.18.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from insightface) (2.32.5)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from insightface) (1.15.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from insightface) (1.2.2)\nRequirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from insightface) (0.25.2)\nRequirement already satisfied: easydict in /usr/local/lib/python3.11/dist-packages (from insightface) (1.13)\nRequirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (from insightface) (3.0.12)\nRequirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (from insightface) (2.0.8)\nRequirement already satisfied: prettytable in /usr/local/lib/python3.11/dist-packages (from insightface) (3.16.0)\nRequirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (15.0.1)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (25.2.10)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (6.33.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (1.13.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib==3.8.4) (1.17.0)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations->insightface) (6.0.3)\nRequirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations->insightface) (2.12.4)\nRequirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations->insightface) (0.0.24)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations->insightface) (4.12.0.88)\nRequirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations->insightface) (3.12.5)\nRequirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations->insightface) (6.5.0)\nRequirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime-gpu) (10.0)\nRequirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx->insightface) (4.15.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prettytable->insightface) (0.2.13)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->insightface) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->insightface) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->insightface) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->insightface) (2025.10.5)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (3.5)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (2.37.0)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (2025.6.11)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (0.4)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->insightface) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->insightface) (3.6.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations->insightface) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations->insightface) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations->insightface) (0.4.2)\nDownloading matplotlib-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hUsing cached onnxruntime_gpu-1.23.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (300.5 MB)\nInstalling collected packages: onnxruntime-gpu, matplotlib, insightface\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed insightface-0.7.3 matplotlib-3.8.4 onnxruntime-gpu-1.23.2\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install insightface\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T09:07:28.581716Z","iopub.execute_input":"2025-12-09T09:07:28.582221Z","execution_failed":"2025-12-09T09:07:31.458Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: insightface in /usr/local/lib/python3.11/dist-packages (0.7.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from insightface) (2.2.6)\nRequirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from insightface) (1.18.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from insightface) (4.67.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from insightface) (2.32.5)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from insightface) (3.8.4)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from insightface) (11.3.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from insightface) (1.15.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from insightface) (1.2.2)\nRequirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from insightface) (0.25.2)\nRequirement already satisfied: easydict in /usr/local/lib/python3.11/dist-packages (from insightface) (1.13)\nRequirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (from insightface) (3.0.12)\nRequirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (from insightface) (2.0.8)\nRequirement already satisfied: prettytable in /usr/local/lib/python3.11/dist-packages (from insightface) (3.16.0)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations->insightface) (6.0.3)\nRequirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations->insightface) (2.12.4)\nRequirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations->insightface) (0.0.24)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations->insightface) (4.12.0.88)\nRequirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations->insightface) (3.12.5)\nRequirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations->insightface) (6.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (25.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (2.9.0.post0)\nRequirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx->insightface) (6.33.0)\nRequirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx->insightface) (4.15.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prettytable->insightface) (0.2.13)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->insightface) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->insightface) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->insightface) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->insightface) (2025.10.5)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (3.5)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (2.37.0)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (2025.6.11)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (0.4)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->insightface) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->insightface) (3.6.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations->insightface) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations->insightface) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations->insightface) (0.4.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->insightface) (1.17.0)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nos.kill(os.getpid(), 9)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-09T09:07:31.458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------\n# ğŸ”¹ Initialisation InsightFace\n# -------------------------------\n\nimport os\nimport cv2\nimport shutil\nimport pandas as pd\nfrom tqdm import tqdm\nfrom insightface.app import FaceAnalysis\n\n# -------------------------------\n# âš™ï¸ ParamÃ¨tres\n# -------------------------------\ndataset_root = \"/kaggle/input/cacd-filtered-dataset/cacd_split/cacd_split\"\noutput_root = \"/kaggle/working/cacd_cleaned_dataset_fast\"\ncsv_file = os.path.join(output_root, \"cacd_cleaned_dataset_fast.csv\")\n\n# Tranches d'Ã¢ge pour StarGAN\nage_groups = {\n    \"trainA\": (0, 24),    # jeunes <25\n    \"trainB\": (25, 49),   # adultes 25-49\n    \"trainC\": (50, 150)   # vieux â‰¥50\n}\n\nos.makedirs(output_root, exist_ok=True)\nfor group in age_groups.keys():\n    os.makedirs(os.path.join(output_root, group), exist_ok=True)\n\n\nprint(\"ğŸš€ Chargement du modÃ¨le InsightFace Age...\")\napp = FaceAnalysis(providers=['CUDAExecutionProvider'])  # GPU\napp.prepare(ctx_id=0)  # ctx_id=0 pour GPU 0\nprint(\"âœ… ModÃ¨le prÃªt !\")\n\n# -------------------------------\n# ğŸ”¹ Fonction de traitement d'une image\n# -------------------------------\ndef process_image(star_folder, img_file):\n    folder_path = os.path.join(dataset_root, star_folder)\n    img_path = os.path.join(folder_path, img_file)\n\n    img = cv2.imread(img_path)\n    if img is None:\n        return None\n\n    faces = app.get(img)\n    if not faces:\n        return None  # pas de visage dÃ©tectÃ©\n\n    age_pred = int(faces[0].age)\n\n    # DÃ©terminer tranche d'Ã¢ge\n    for group, (min_age, max_age) in age_groups.items():\n        if min_age <= age_pred <= max_age:\n            group_folder = os.path.join(output_root, group, star_folder)\n            os.makedirs(group_folder, exist_ok=True)\n            new_name = f\"{age_pred}_{star_folder}_{img_file.split('_')[-1]}\"\n            new_path = os.path.join(group_folder, new_name)\n            shutil.copy(img_path, new_path)\n            return {\n                \"star\": star_folder,\n                \"original_image\": img_file,\n                \"predicted_age\": age_pred,\n                \"new_name\": new_name,\n                \"age_group\": group\n            }\n    return None\n\n# -------------------------------\n# ğŸ”¹ PrÃ©parer toutes les images\n# -------------------------------\nall_images = []\nfor star_folder in os.listdir(dataset_root):\n    folder_path = os.path.join(dataset_root, star_folder)\n    if not os.path.isdir(folder_path):\n        continue\n    for img_file in os.listdir(folder_path):\n        if img_file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n            all_images.append((star_folder, img_file))\n\nprint(f\"ğŸ“Š Total images Ã  traiter : {len(all_images)}\")\n\n\n\n# -------------------------------\n# ğŸ”¹ Traitement sÃ©quentiel (GPU)\n# -------------------------------\nresults = []\nfor star_folder, img_file in tqdm(all_images, total=len(all_images)):\n    r = process_image(star_folder, img_file)\n    if r is not None:\n        results.append(r)\n\n# -------------------------------\n# ğŸ”¹ GÃ©nÃ©ration du CSV\n# -------------------------------\ndf = pd.DataFrame(results)\ndf.to_csv(csv_file, index=False)\nprint(f\"âœ… Dataset nettoyÃ© et CSV crÃ©Ã© : {csv_file}\")\n\n# -------------------------------\n# ğŸ”¹ Stats rapides\n# -------------------------------\nprint(\"\\nğŸ“Š Stats par tranche d'Ã¢ge :\")\nfor group in age_groups.keys():\n    count = len(df[df['age_group'] == group])\n    print(f\"{group}: {count} images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T09:07:46.404938Z","iopub.execute_input":"2025-12-09T09:07:46.405611Z","iopub.status.idle":"2025-12-09T10:52:26.938111Z","shell.execute_reply.started":"2025-12-09T09:07:46.405586Z","shell.execute_reply":"2025-12-09T10:52:26.937304Z"}},"outputs":[{"name":"stdout","text":"ğŸš€ Chargement du modÃ¨le InsightFace Age...\ndownload_path: /root/.insightface/models/buffalo_l\nDownloading /root/.insightface/models/buffalo_l.zip from https://github.com/deepinsight/insightface/releases/download/v0.7/buffalo_l.zip...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 281857/281857 [00:02<00:00, 104013.49KB/s]\n","output_type":"stream"},{"name":"stdout","text":"Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\nfind model: /root/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\nApplied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\nfind model: /root/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\nApplied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\nfind model: /root/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\nApplied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\nfind model: /root/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\nApplied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\nfind model: /root/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\nset det-size: (640, 640)\nâœ… ModÃ¨le prÃªt !\nğŸ“Š Total images Ã  traiter : 154915\n","output_type":"stream"},{"name":"stderr","text":" 10%|â–ˆ         | 16049/154915 [10:42<1:34:10, 24.58it/s]Premature end of JPEG file\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 154915/154915 [1:43:59<00:00, 24.83it/s]  \n","output_type":"stream"},{"name":"stdout","text":"âœ… Dataset nettoyÃ© et CSV crÃ©Ã© : /kaggle/working/cacd_cleaned_dataset_fast/cacd_cleaned_dataset_fast.csv\n\nğŸ“Š Stats par tranche d'Ã¢ge :\ntrainA: 4774 images\ntrainB: 103020 images\ntrainC: 45653 images\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\n# ==============================================\n# ğŸ”¹ Filtrer les stars prÃ©sentes dans les 3 tranches d'Ã¢ge\n# ==============================================\n\nimport pandas as pd\nimport os\nimport shutil\nfrom tqdm import tqdm\n\n# -------------------------------\n# âš™ï¸ ParamÃ¨tres\n# -------------------------------\ninput_root = \"/kaggle/working/cacd_cleaned_dataset_fast\"\noutput_root = \"/kaggle/working/cacd_complete_stars_dataset\"\ninput_csv = os.path.join(input_root, \"cacd_cleaned_dataset_fast.csv\")\noutput_csv = os.path.join(output_root, \"cacd_complete_stars_dataset.csv\")\n\nage_groups = [\"trainA\", \"trainB\", \"trainC\"]\n\nos.makedirs(output_root, exist_ok=True)\nfor group in age_groups:\n    os.makedirs(os.path.join(output_root, group), exist_ok=True)\n\n# -------------------------------\n# ğŸ”¹ Charger le CSV existant\n# -------------------------------\ndf = pd.read_csv(input_csv)\n\n# -------------------------------\n# ğŸ”¹ Identifier les stars complÃ¨tes\n# -------------------------------\nstars_complete = []\n\nfor star, group_df in df.groupby('star'):\n    star_groups = set(group_df['age_group'].unique())\n    if all(g in star_groups for g in age_groups):\n        stars_complete.append(star)\n\nprint(f\"â­ Nombre de stars avec images dans les 3 tranches : {len(stars_complete)}\")\n\n# -------------------------------\n# ğŸ”¹ Filtrer le DataFrame\n# -------------------------------\ndf_filtered = df[df['star'].isin(stars_complete)].copy()\n\n# -------------------------------\n# ğŸ”¹ Copier les images dans le nouveau dossier\n# -------------------------------\nfor _, row in tqdm(df_filtered.iterrows(), total=len(df_filtered), desc=\"Copie images\"):\n    src_path = os.path.join(input_root, row['age_group'], row['star'], row['new_name'])\n    dest_folder = os.path.join(output_root, row['age_group'], row['star'])\n    os.makedirs(dest_folder, exist_ok=True)\n    dest_path = os.path.join(dest_folder, row['new_name'])\n    if not os.path.exists(dest_path):\n        shutil.copy(src_path, dest_path)\n\n# -------------------------------\n# ğŸ”¹ Enregistrer le CSV final\n# -------------------------------\ndf_filtered.to_csv(output_csv, index=False)\nprint(f\"âœ… CSV des stars complÃ¨tes crÃ©Ã© : {output_csv}\")\n\n# -------------------------------\n# ğŸ”¹ Stats rapides\n# -------------------------------\nprint(\"\\nğŸ“Š Stats par tranche d'Ã¢ge pour stars complÃ¨tes :\")\nfor group in age_groups:\n    count = len(df_filtered[df_filtered['age_group'] == group])\n    print(f\"{group}: {count} images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T10:52:26.939698Z","iopub.execute_input":"2025-12-09T10:52:26.940195Z","iopub.status.idle":"2025-12-09T10:52:44.425845Z","shell.execute_reply.started":"2025-12-09T10:52:26.940175Z","shell.execute_reply":"2025-12-09T10:52:44.425219Z"}},"outputs":[{"name":"stdout","text":"â­ Nombre de stars avec images dans les 3 tranches : 1012\n","output_type":"stream"},{"name":"stderr","text":"Copie images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77685/77685 [00:16<00:00, 4612.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"âœ… CSV des stars complÃ¨tes crÃ©Ã© : /kaggle/working/cacd_complete_stars_dataset/cacd_complete_stars_dataset.csv\n\nğŸ“Š Stats par tranche d'Ã¢ge pour stars complÃ¨tes :\ntrainA: 3864 images\ntrainB: 57037 images\ntrainC: 16784 images\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nfrom PIL import Image\n\nroot = \"/kaggle/working/cacd_complete_stars_dataset\"\nsplits = ['trainA','trainB','trainC']\n\ncorrupted = []\n\nfor sp in splits:\n    sp_path = os.path.join(root, sp)\n    if not os.path.isdir(sp_path):\n        continue\n    for sub in os.listdir(sp_path):\n        sub_path = os.path.join(sp_path, sub)\n        if not os.path.isdir(sub_path):\n            continue\n        for f in os.listdir(sub_path):\n            f_path = os.path.join(sub_path, f)\n            try:\n                img = Image.open(f_path)\n                img.verify()  # VÃ©rifie si l'image est correcte\n            except:\n                corrupted.append(f_path)\n\nprint(f\"Images corrompues trouvÃ©es: {len(corrupted)}\")\nfor f in corrupted:\n    os.remove(f)\n    print(f\"SupprimÃ©e: {f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T10:52:44.426529Z","iopub.execute_input":"2025-12-09T10:52:44.426715Z","iopub.status.idle":"2025-12-09T10:52:48.713845Z","shell.execute_reply.started":"2025-12-09T10:52:44.426701Z","shell.execute_reply":"2025-12-09T10:52:48.713223Z"}},"outputs":[{"name":"stdout","text":"Images corrompues trouvÃ©es: 0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\nimport shutil\nimport os\n\nsamples_dir = \"/kaggle/working/outputs/samples\"\n\nif os.path.exists(samples_dir):\n    # Supprime tout le contenu du dossier samples\n    for f in os.listdir(samples_dir):\n        f_path = os.path.join(samples_dir, f)\n        if os.path.isdir(f_path):\n            shutil.rmtree(f_path)\n        else:\n            os.remove(f_path)\n    print(f\"âœ… Tout le contenu de {samples_dir} a Ã©tÃ© supprimÃ©\")\nelse:\n    print(f\"âš ï¸ Le dossier {samples_dir} n'existe pas\")\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T10:52:48.715322Z","iopub.execute_input":"2025-12-09T10:52:48.715533Z","iopub.status.idle":"2025-12-09T10:52:48.720565Z","shell.execute_reply.started":"2025-12-09T10:52:48.715518Z","shell.execute_reply":"2025-12-09T10:52:48.719779Z"}},"outputs":[{"name":"stdout","text":"âš ï¸ Le dossier /kaggle/working/outputs/samples n'existe pas\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ====================== STAR GAN V2 - Version OPTIMISÃ‰E (NettetÃ© + Cheveux Gris + 40k iters) ======================\nimport os, random, glob\nfrom collections import Counter\nfrom tqdm import tqdm\nfrom PIL import Image, ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nfrom torch.amp import autocast, GradScaler\n\n# ====================== CONFIG OPTIMISÃ‰E ======================\nCFG = {\n    \"data_root\": \"/kaggle/working/cacd_complete_stars_dataset\",\n    \"out_dir\": \"/kaggle/working/starganv2_optimized\",\n    \"iters\": 40000,  # +10k\n    \"img_size\": 128,\n    \"batch_size\": 28,\n    \"num_workers\": 4,\n    \"lr\": 1.5e-4,\n    \"latent_dim\": 16,\n    \"style_dim\": 64,\n    \"n_domains\": 3,  # 0=jeune, 1=adulte, 2=Ã¢gÃ©\n    \"n_res\": 4,\n    \"lambda_cyc\": 5.0,\n    \"lambda_cls\": 1.0,\n    \"lambda_sty\": 0.1,\n    \"lambda_reg\": 0.0001,\n    \"sample_every\": 500,\n    \"save_every\": 5000,\n    \"seed\": 42,\n    \"sharpen_alpha\": 0.3,\n    \"gray_hair_intensity\": 0.3\n}\n\n# ====================== SEED ======================\ndef seed_everything(seed=42):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(CFG[\"seed\"])\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nos.makedirs(CFG[\"out_dir\"] + \"/samples\", exist_ok=True)\nos.makedirs(CFG[\"out_dir\"] + \"/checkpoints\", exist_ok=True)\n\n# ====================== UTILS ======================\ndef sharpen(img_tensor, alpha=0.3):\n    kernel = torch.tensor([[0, -1, 0], [-1, 5, -1], [0, -1, 0]], dtype=torch.float32, device=img_tensor.device)\n    kernel = kernel.unsqueeze(0).unsqueeze(0)\n    img_sharp = F.conv2d(img_tensor.unsqueeze(0), kernel.repeat(3,1,1,1), padding=1, groups=3)\n    return torch.clamp((1-alpha)*img_tensor + alpha*img_sharp.squeeze(0), -1, 1)\n\ndef add_gray_hair(img_tensor, domain, intensity=0.3):\n    if domain.item() == 2:  # domaine Ã‚GÃ‰\n        gray = img_tensor.mean(dim=1, keepdim=True)\n        img_tensor = img_tensor * (1-intensity) + gray * intensity\n    return img_tensor\n\n# ====================== DATASET ======================\nclass AgeDataset(Dataset):\n    def __init__(self, root, img_size=128):\n        self.samples = []\n        self.domain_names = [\"trainA\", \"trainB\", \"trainC\"]\n        for d_idx, folder in enumerate(self.domain_names):\n            path = os.path.join(root, folder)\n            if not os.path.isdir(path): continue\n            for person in os.listdir(path):\n                person_path = os.path.join(path, person)\n                if not os.path.isdir(person_path): continue\n                for file in glob.glob(os.path.join(person_path, \"*.*\")):\n                    if file.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\")):\n                        self.samples.append((file, d_idx))\n        print(f\"Dataset chargÃ© : {len(self.samples)} images\")\n        print(f\"RÃ©partition des domaines: {Counter([d for _,d in self.samples])}\")\n\n        self.transform = transforms.Compose([\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5]*3, [0.5]*3)\n        ])\n\n    def __len__(self): return len(self.samples)\n    def __getitem__(self, idx):\n        path, domain = self.samples[idx]\n        try:\n            img = Image.open(path).convert(\"RGB\")\n        except:\n            img = Image.new(\"RGB\", (CFG[\"img_size\"], CFG[\"img_size\"]))\n        return self.transform(img), domain, path\n\n# ====================== MODELS ======================\ndef weights_init(m):\n    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n        nn.init.normal_(m.weight, 0.0, 0.02)\n        if hasattr(m, 'bias') and m.bias is not None:\n            nn.init.constant_(m.bias, 0)\n\nclass AdaIN(nn.Module):\n    def __init__(self, channels, style_dim):\n        super().__init__()\n        self.norm = nn.InstanceNorm2d(channels, affine=False)\n        self.style = nn.Linear(style_dim, channels * 2)\n    def forward(self, x, s):\n        style = self.style(s)\n        gamma, beta = style.chunk(2, 1)\n        gamma = gamma.unsqueeze(-1).unsqueeze(-1)\n        beta = beta.unsqueeze(-1).unsqueeze(-1)\n        return gamma * self.norm(x) + beta\n\nclass ResBlock(nn.Module):\n    def __init__(self, channels, style_dim):\n        super().__init__()\n        self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\n        self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\n        self.adain1 = AdaIN(channels, style_dim)\n        self.adain2 = AdaIN(channels, style_dim)\n    def forward(self, x, s):\n        h = self.conv1(x)\n        h = self.adain1(h, s)\n        h = F.relu(h, inplace=True)\n        h = self.conv2(h)\n        h = self.adain2(h, s)\n        return x + h\n\nclass Generator(nn.Module):\n    def __init__(self, style_dim=64, n_res=4):\n        super().__init__()\n        self.enc = nn.Sequential(\n            nn.Conv2d(3, 64, 7, 1, 3, bias=False), nn.ReLU(True),\n            nn.Conv2d(64, 128, 4, 2, 1, bias=False), nn.ReLU(True),\n            nn.Conv2d(128, 256, 4, 2, 1, bias=False), nn.ReLU(True),\n        )\n        self.res = nn.ModuleList([ResBlock(256, style_dim) for _ in range(n_res)])\n        self.dec = nn.Sequential(\n            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False), nn.ReLU(True),\n            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False), nn.ReLU(True),\n            nn.Conv2d(64, 3, 7, 1, 3), nn.Tanh()\n        )\n    def forward(self, x, s):\n        h = self.enc(x)\n        for block in self.res:\n            h = block(h, s)\n        return self.dec(h)\n\nclass MappingNetwork(nn.Module):\n    def __init__(self, latent_dim=16, style_dim=64, n_domains=3):\n        super().__init__()\n        self.n_domains = n_domains\n        self.shared = nn.Sequential(\n            nn.Linear(latent_dim, 256), nn.ReLU(True),\n            nn.Linear(256, 256), nn.ReLU(True),\n        )\n        self.branches = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(256, 256), nn.ReLU(True),\n                nn.Linear(256, style_dim)\n            ) for _ in range(n_domains)\n        ])\n    def forward(self, z, domain):\n        h = self.shared(z)\n        styles = [branch(h) for branch in self.branches]\n        styles = torch.stack(styles, dim=1)\n        batch_size = z.size(0)\n        return styles[torch.arange(batch_size), domain]\n\nclass StyleEncoder(nn.Module):\n    def __init__(self, style_dim=64, n_domains=3):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(3, 64, 7, 1, 3), nn.ReLU(True),\n            nn.Conv2d(64, 128, 4, 2, 1), nn.ReLU(True),\n            nn.Conv2d(128, 256, 4, 2, 1), nn.ReLU(True),\n            nn.AdaptiveAvgPool2d(1),\n        )\n        self.style = nn.Linear(256, style_dim)\n        self.classifier = nn.Linear(256, n_domains)\n    def forward(self, x):\n        h = self.net(x).view(x.size(0), -1)\n        style = self.style(h)\n        cls_output = self.classifier(h)\n        return style, cls_output\n\nclass Discriminator(nn.Module):\n    def __init__(self, n_domains=3):\n        super().__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(3, 64, 4, 2, 1), nn.LeakyReLU(0.2, True),\n            nn.Conv2d(64, 128, 4, 2, 1), nn.LeakyReLU(0.2, True),\n            nn.Conv2d(128, 256, 4, 2, 1), nn.LeakyReLU(0.2, True),\n            nn.Conv2d(256, 512, 4, 2, 1), nn.LeakyReLU(0.2, True),\n            nn.Conv2d(512, 1024, 4, 1, 0), nn.LeakyReLU(0.2, True),\n        )\n        self.src = nn.Conv2d(1024, 1, 3, 1, 1)\n        self.cls = nn.Conv2d(1024, n_domains, 3, 1, 1)\n    def forward(self, x):\n        h = self.main(x)\n        src = self.src(h).view(x.size(0), -1)\n        cls = self.cls(h).mean([2,3])\n        return src, cls\n\n# ====================== DATA & MODELS ======================\ndataset = AgeDataset(CFG[\"data_root\"])\ncounts = Counter([d for _,d in dataset.samples])\nweights = [1.0 / counts[d] for _,d in dataset.samples]\nsampler = WeightedRandomSampler(weights, len(weights), replacement=True)\nloader = DataLoader(dataset, batch_size=CFG[\"batch_size\"], sampler=sampler,\n                    num_workers=CFG[\"num_workers\"], drop_last=True,\n                    pin_memory=True, persistent_workers=True)\n\nG = Generator(CFG[\"style_dim\"], CFG[\"n_res\"]).to(device)\nF_map = MappingNetwork(CFG[\"latent_dim\"], CFG[\"style_dim\"], CFG[\"n_domains\"]).to(device)\nF_style = StyleEncoder(CFG[\"style_dim\"], CFG[\"n_domains\"]).to(device)\nD = Discriminator(CFG[\"n_domains\"]).to(device)\n\nG.apply(weights_init); F_map.apply(weights_init); F_style.apply(weights_init); D.apply(weights_init)\n\nopt_g = torch.optim.Adam(list(G.parameters()) + list(F_map.parameters()) + list(F_style.parameters()), \n                         lr=CFG[\"lr\"], betas=(0.5, 0.999))\nopt_d = torch.optim.Adam(D.parameters(), lr=CFG[\"lr\"], betas=(0.5, 0.999))\nscaler_g = GradScaler()\nscaler_d = GradScaler()\n\n# ====================== TRAINING LOOP ======================\npbar = tqdm(total=CFG[\"iters\"])\ndomain_names = [\"JEUNE\", \"ADULTE\", \"Ã‚GÃ‰\"]\nloader_iter = iter(loader)\n\nfor it in range(1, CFG[\"iters\"] + 1):\n    try:\n        real, real_domain, _ = next(loader_iter)\n    except StopIteration:\n        loader_iter = iter(loader)\n        real, real_domain, _ = next(loader_iter)\n\n    real = real.to(device, non_blocking=True)\n    real_domain = real_domain.to(device, non_blocking=True)\n    b = real.size(0)\n\n    # --- reste de la boucle unchanged ---\n\n    # --- Discriminator ---\n    opt_d.zero_grad()\n    with autocast('cuda'):\n        tgt_domain = torch.randint(0, CFG[\"n_domains\"], (b,), device=device)\n        z = torch.randn(b, CFG[\"latent_dim\"], device=device)\n        style = F_map(z, tgt_domain)\n        fake = G(real, style).detach()\n\n        real_src, real_cls = D(real)\n        fake_src, _ = D(fake)\n\n        loss_d_adv = F.softplus(-real_src).mean() + F.softplus(fake_src).mean()\n        loss_d_cls = F.cross_entropy(real_cls, real_domain)\n        loss_d = loss_d_adv + CFG[\"lambda_cls\"] * loss_d_cls\n\n    scaler_d.scale(loss_d).backward()\n    scaler_d.step(opt_d)\n    scaler_d.update()\n\n    # --- Generator ---\n    opt_g.zero_grad()\n    with autocast('cuda'):\n        tgt_domain = torch.randint(0, CFG[\"n_domains\"], (b,), device=device)\n        z = torch.randn(b, CFG[\"latent_dim\"], device=device)\n        style = F_map(z, tgt_domain)\n        fake = G(real, style)\n        fake_src, fake_cls = D(fake)\n\n        loss_g_adv = -fake_src.mean()\n        loss_g_cls = F.cross_entropy(fake_cls, tgt_domain)\n\n        style_orig, _ = F_style(real)\n        rec = G(fake, style_orig)\n        loss_cyc = F.l1_loss(rec, real)\n\n        z2 = torch.randn(b, CFG[\"latent_dim\"], device=device)\n        style2 = F_map(z2, tgt_domain)\n        fake2 = G(real, style2)\n        loss_sty = -F.l1_loss(fake, fake2) * CFG[\"lambda_sty\"]\n\n        style_reg = F.l1_loss(style, torch.zeros_like(style)) * CFG[\"lambda_reg\"]\n\n        loss_g = (loss_g_adv + CFG[\"lambda_cls\"] * loss_g_cls + \n                 CFG[\"lambda_cyc\"] * loss_cyc + loss_sty + style_reg)\n\n    scaler_g.scale(loss_g).backward()\n    scaler_g.step(opt_g)\n    scaler_g.update()\n\n    # --- SAMPLING DIAGNOSTIQUE ---\n    if it % CFG[\"sample_every\"] == 0:\n        G.eval(); F_map.eval(); F_style.eval()\n        with torch.no_grad():\n            ref_images, ref_domains_list = [], []\n            for domain in range(CFG[\"n_domains\"]):\n                domain_mask = (real_domain == domain)\n                if domain_mask.sum() > 0:\n                    idx = torch.where(domain_mask)[0][0]\n                    ref_images.append(real[idx])\n                    ref_domains_list.append(domain)\n            \n            if len(ref_images) >= 2:\n                current_ref = torch.stack(ref_images[:min(4, len(ref_images))])\n                ref_domains = torch.tensor(ref_domains_list[:current_ref.size(0)], device=device)\n                \n                outs = [current_ref.cpu()]\n                \n                for target_domain in range(CFG[\"n_domains\"]):\n                    tgt_domains = torch.full((current_ref.size(0),), target_domain, device=device)\n                    z = torch.randn(current_ref.size(0), CFG[\"latent_dim\"], device=device)\n                    s = F_map(z, tgt_domains)\n                    gen = G(current_ref, s)\n                    # Appliquer cheveux gris et nettetÃ©\n                    for i in range(gen.size(0)):\n                        gen[i] = add_gray_hair(gen[i], tgt_domains[i], CFG[\"gray_hair_intensity\"])\n                        gen[i] = sharpen(gen[i], CFG[\"sharpen_alpha\"])\n                    outs.append(gen.cpu())\n\n                grid = torch.cat(outs, dim=0)\n                save_image(grid * 0.5 + 0.5, \n                          f\"{CFG['out_dir']}/samples/{it:06d}.jpg\", \n                          nrow=current_ref.size(0), padding=4, pad_value=1)\n                \n                ref_names = [domain_names[d] for d in ref_domains.cpu()]\n                print(f\"\\nğŸ­ Sample {it}: {ref_names} â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\")\n\n        G.train(); F_map.train(); F_style.train()\n\n       # --- Checkpoint (poids seulement pendant le training) ---\n    if it % CFG[\"save_every\"] == 0:\n        torch.save({\n            \"G\": G.state_dict(),\n            \"F_map\": F_map.state_dict(),\n            \"F_style\": F_style.state_dict(),\n            \"D\": D.state_dict(),\n            \"opt_g\": opt_g.state_dict(),\n            \"opt_d\": opt_d.state_dict(),\n            \"iter\": it,\n            \"CFG\": CFG\n        }, f\"{CFG['out_dir']}/checkpoints/checkpoint_{it}.pth\")\n\n    # --- Affichage progression ---\n    pbar.set_postfix({\n        \"D\": f\"{loss_d.item():.4f}\", \n        \"G\": f\"{loss_g.item():.4f}\",\n        \"Cyc\": f\"{loss_cyc.item():.4f}\",\n        \"Sty\": f\"{loss_sty.item():.4f}\"\n    })\n    pbar.update(1)\n\n# ====================== FIN TRAINING ======================\nprint(\"ğŸ‰ Training terminÃ© !\")\n\n# --- Sauvegarde MODELE COMPLET pour AGE & YOUNG ---\ntorch.save({\n    \"G\": G,\n    \"F_map\": F_map, \n    \"F_style\": F_style,\n    \"D\": D,\n    \"CFG\": CFG,\n    \"target_domain\": 2  # Ã‚GÃ‰\n}, f\"{CFG['out_dir']}/final_model_age.pth\")\n\ntorch.save({\n    \"G\": G,\n    \"F_map\": F_map, \n    \"F_style\": F_style,\n    \"D\": D,\n    \"CFG\": CFG,\n    \"target_domain\": 0  # JEUNE\n}, f\"{CFG['out_dir']}/final_model_young.pth\")\n\nprint(\"âœ… ModÃ¨les sauvegardÃ©s : final_model_age.pth & final_model_young.pth\")\n\n# ====================== CrÃ©er ZIP pour tÃ©lÃ©chargement facile (Kaggle) ======================\nimport shutil\nshutil.make_archive(\n    base_name=f\"{CFG['out_dir']}/starganv2_models\",\n    format=\"zip\",\n    root_dir=CFG[\"out_dir\"],\n    base_dir=\".\"\n)\nprint(\"ğŸ“¦ Archive prÃªte : starganv2_models.zip\")\n\n# ====================== TEST FINAL ======================\nprint(\"\\nğŸ§ª Test final des transformations...\")\nG.eval(); F_map.eval()\nwith torch.no_grad():\n    test_img = real[0:1]\n    source_domain = real_domain[0].item()\n    print(f\"Image source: {domain_names[source_domain]}\")\n    \n    for target_domain in range(CFG[\"n_domains\"]):\n        if target_domain != source_domain:\n            z = torch.randn(1, CFG[\"latent_dim\"], device=device)\n            style = F_map(z, torch.tensor([target_domain], device=device))\n            result = G(test_img, style)\n            result = add_gray_hair(result[0], torch.tensor(target_domain), CFG[\"gray_hair_intensity\"]).unsqueeze(0)\n            result = sharpen(result[0], CFG[\"sharpen_alpha\"]).unsqueeze(0)\n            save_image(result * 0.5 + 0.5, \n                      f\"{CFG['out_dir']}/test_{domain_names[source_domain]}_to_{domain_names[target_domain]}.jpg\")\n            print(f\"  â†’ {domain_names[target_domain]} : sauvegardÃ©\")\n\nprint(\"âœ… Tous les tests sont terminÃ©s !\")\n# ====================== FIN TRAINING ======================\nprint(\"ğŸ‰ Training terminÃ© !\")\n\n\n# ====================== MODÃˆLE D'EXPORT POUR INFERENCE ======================\nclass StarGANv2AgeModel(nn.Module):\n    def __init__(self, G, F_map, F_style, CFG):\n        super().__init__()\n        self.G = G\n        self.F_map = F_map\n        self.F_style = F_style\n        self.CFG = CFG\n\n    @torch.no_grad()\n    def inference(self, img_tensor, target_domain, device):\n        \"\"\"\n        img_tensor : (1, 3, 128, 128)\n        target_domain : 0=jeune, 1=adulte, 2=Ã¢gÃ©\n        \"\"\"\n        z = torch.randn(1, self.CFG[\"latent_dim\"], device=device)\n        target_domain = torch.tensor([target_domain], device=device)\n        style = self.F_map(z, target_domain)\n        output = self.G(img_tensor.to(device), style)\n        return output\n\n\n# ====================== CRÃ‰ATION DES DEUX MODÃˆLES EXPORTABLES ======================\nfinal_age_model = StarGANv2AgeModel(G, F_map, F_style, CFG)\nfinal_young_model = StarGANv2AgeModel(G, F_map, F_style, CFG)\n\n\n# ====================== SAUVEGARDE FORMAT FLASK ======================\ntorch.save(final_age_model, f\"{CFG['out_dir']}/age_model.pth\")\ntorch.save(final_young_model, f\"{CFG['out_dir']}/young_model.pth\")\n\nprint(\"ğŸ‰ Export propre terminÃ© : age_model.pth et young_model.pth prÃªts Ã  lâ€™emploi !\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-09T09:07:31.458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================== SAUVEGARDE FLASK COMPATIBLE ======================\n\nclass StarGANWrapper(nn.Module):\n    def __init__(self, G, F_map, target_domain, cfg):\n        super().__init__()\n        self.G = G\n        self.F_map = F_map\n        self.target_domain = target_domain\n        self.cfg = cfg\n\n    def forward(self, x):\n        b = x.size(0)\n        z = torch.randn(b, self.cfg[\"latent_dim\"]).to(x.device)\n        domain = torch.full((b,), self.target_domain, device=x.device, dtype=torch.long)\n        style = self.F_map(z, domain)\n        return self.G(x, style)\n\n# ----- MODELE AGE -----\nage_model = StarGANWrapper(G, F_map, target_domain=2, cfg=CFG)\nage_model.eval()\n\ntorch.save(age_model, f\"{CFG['out_dir']}/final_model_age.pth\")\n\n# ----- MODELE JEUNE -----\nyoung_model = StarGANWrapper(G, F_map, target_domain=0, cfg=CFG)\nyoung_model.eval()\n\ntorch.save(young_model, f\"{CFG['out_dir']}/final_model_young.pth\")\n\nprint(\"âœ… ModÃ¨les Flask prÃªts : final_model_age.pth et final_model_young.pth sauvegardÃ©s !\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-09T09:07:31.458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom PIL import Image\nimport io\nimport base64\n\n# ====================== CLASSES ======================\nclass AdaIN(nn.Module):\n    def __init__(self, channels, style_dim):\n        super().__init__()\n        self.norm = nn.InstanceNorm2d(channels, affine=False)\n        self.style = nn.Linear(style_dim, channels * 2)\n    def forward(self, x, s):\n        style = self.style(s)\n        gamma, beta = style.chunk(2, 1)\n        gamma = gamma.unsqueeze(-1).unsqueeze(-1)\n        beta = beta.unsqueeze(-1).unsqueeze(-1)\n        return gamma * self.norm(x) + beta\n\nclass ResBlock(nn.Module):\n    def __init__(self, channels, style_dim):\n        super().__init__()\n        self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\n        self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\n        self.adain1 = AdaIN(channels, style_dim)\n        self.adain2 = AdaIN(channels, style_dim)\n    def forward(self, x, s):\n        h = self.conv1(x)\n        h = self.adain1(h, s)\n        h = F.relu(h, inplace=True)\n        h = self.conv2(h)\n        h = self.adain2(h, s)\n        return x + h\n\nclass Generator(nn.Module):\n    def __init__(self, style_dim=64, n_res=4):\n        super().__init__()\n        self.enc = nn.Sequential(\n            nn.Conv2d(3, 64, 7, 1, 3, bias=False), nn.ReLU(True),\n            nn.Conv2d(64, 128, 4, 2, 1, bias=False), nn.ReLU(True),\n            nn.Conv2d(128, 256, 4, 2, 1, bias=False), nn.ReLU(True),\n        )\n        self.res = nn.ModuleList([ResBlock(256, style_dim) for _ in range(n_res)])\n        self.dec = nn.Sequential(\n            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False), nn.ReLU(True),\n            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False), nn.ReLU(True),\n            nn.Conv2d(64, 3, 7, 1, 3), nn.Tanh()\n        )\n    def forward(self, x, s):\n        h = self.enc(x)\n        for block in self.res:\n            h = block(h, s)\n        return self.dec(h)\n\nclass MappingNetwork(nn.Module):\n    def __init__(self, latent_dim=16, style_dim=64, n_domains=3):\n        super().__init__()\n        self.n_domains = n_domains\n        self.shared = nn.Sequential(\n            nn.Linear(latent_dim, 256), nn.ReLU(True),\n            nn.Linear(256, 256), nn.ReLU(True),\n        )\n        self.branches = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(256, 256), nn.ReLU(True),\n                nn.Linear(256, style_dim)\n            ) for _ in range(n_domains)\n        ])\n    def forward(self, z, domain):\n        h = self.shared(z)\n        styles = [branch(h) for branch in self.branches]\n        styles = torch.stack(styles, dim=1)\n        batch_size = z.size(0)\n        return styles[torch.arange(batch_size), domain]\n\n# Wrapper pour utiliser directement\nclass StarGANWrapper(nn.Module):\n    def __init__(self, G, F_map, target_domain, cfg):\n        super().__init__()\n        self.G = G\n        self.F_map = F_map\n        self.target_domain = target_domain\n        self.cfg = cfg\n    def forward(self, x):\n        b = x.size(0)\n        z = torch.randn(b, self.cfg[\"latent_dim\"]).to(x.device)\n        domain = torch.full((b,), self.target_domain, device=x.device, dtype=torch.long)\n        style = self.F_map(z, domain)\n        return self.G(x, style)\n\n# ====================== DEVICE ======================\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# ====================== CHEMINS ======================\nyoung_model_path = \"/kaggle/working/starganv2_optimized/final_model_young.pth\"\nage_model_path   = \"/kaggle/working/starganv2_optimized/final_model_age.pth\"\n\n# ====================== CHARGEMENT ======================\nyoung_model = torch.load(young_model_path, map_location=device, weights_only=False)\nage_model   = torch.load(age_model_path, map_location=device, weights_only=False)\n\n# ====================== FONCTIONS ======================\ntransform = transforms.Compose([\n    transforms.Resize((128,128)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])\n\ndef decode_image(image_path):\n    img = Image.open(image_path).convert(\"RGB\")\n    return transform(img).unsqueeze(0).to(device)\n\ndef encode_image(tensor):\n    tensor = tensor.squeeze(0).cpu()\n    tensor = (tensor * 0.5 + 0.5).clamp(0, 1)\n    pil_img = transforms.ToPILImage()(tensor)\n    buffer = io.BytesIO()\n    pil_img.save(buffer, format=\"PNG\")\n    return base64.b64encode(buffer.getvalue()).decode()\n\ndef transform_image(wrapper_model, image_path):\n    wrapper_model.eval()\n    img_tensor = decode_image(image_path)\n    with torch.no_grad():\n        out = wrapper_model(img_tensor)\n    return encode_image(out)\n\n# ====================== UTILISATION ======================\n# Exemple :\n# RÃ©sultat \"jeune\" Ã  partir d'une image\nimg_base64 = transform_image(young_model, \"/kaggle/working/cacd_cleaned_dataset_fast/trainC/AaronAshmore/55_AaronAshmore_0013.jpg\")\n# RÃ©sultat \"Ã¢gÃ©\" Ã  partir de la mÃªme image\nimg_base64_age = transform_image(age_model, \"/kaggle/working/cacd_cleaned_dataset_fast/trainB/AaronPaul/28_AaronPaul_0001.jpg\")\n\n# img_base64 est en base64, tu peux l'afficher dans notebook par :\n# from IPython.display import display, HTML\n# display(HTML(f\"<img src='data:image/png;base64,{img_base64}'/>\"))\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-09T09:07:31.458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ====================== STAR GAN V2 - Version OPTIMISÃ‰E (NettetÃ© + Cheveux Gris + 40k iters) ======================\nimport os, random, glob\nfrom collections import Counter\nfrom tqdm import tqdm\nfrom PIL import Image, ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nfrom torch.amp import autocast, GradScaler\n\n# ====================== CONFIG OPTIMISÃ‰E ======================\nCFG = {\n    \"data_root\": \"/kaggle/working/cacd_complete_stars_dataset\",\n    \"out_dir\": \"/kaggle/working/starganv2_optimized\",\n    \"iters\": 40000,  # +10k\n    \"img_size\": 128,\n    \"batch_size\": 28,\n    \"num_workers\": 4,\n    \"lr\": 1.5e-4,\n    \"latent_dim\": 16,\n    \"style_dim\": 64,\n    \"n_domains\": 3,  # 0=jeune, 1=adulte, 2=Ã¢gÃ©\n    \"n_res\": 4,\n    \"lambda_cyc\": 5.0,\n    \"lambda_cls\": 1.0,\n    \"lambda_sty\": 0.1,\n    \"lambda_reg\": 0.0001,\n    \"sample_every\": 500,\n    \"save_every\": 5000,\n    \"seed\": 42,\n    \"sharpen_alpha\": 0.3,\n    \"gray_hair_intensity\": 0.3\n}\n\n# ====================== SEED ======================\ndef seed_everything(seed=42):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(CFG[\"seed\"])\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nos.makedirs(CFG[\"out_dir\"] + \"/samples\", exist_ok=True)\nos.makedirs(CFG[\"out_dir\"] + \"/checkpoints\", exist_ok=True)\n\n# ====================== UTILS ======================\ndef sharpen(img_tensor, alpha=0.3):\n    kernel = torch.tensor([[0, -1, 0], [-1, 5, -1], [0, -1, 0]], dtype=torch.float32, device=img_tensor.device)\n    kernel = kernel.unsqueeze(0).unsqueeze(0)\n    img_sharp = F.conv2d(img_tensor.unsqueeze(0), kernel.repeat(3,1,1,1), padding=1, groups=3)\n    return torch.clamp((1-alpha)*img_tensor + alpha*img_sharp.squeeze(0), -1, 1)\n\ndef add_gray_hair(img_tensor, domain, intensity=0.3):\n    if domain.item() == 2:  # domaine Ã‚GÃ‰\n        gray = img_tensor.mean(dim=1, keepdim=True)\n        img_tensor = img_tensor * (1-intensity) + gray * intensity\n    return img_tensor\n\n# ====================== DATASET ======================\nclass AgeDataset(Dataset):\n    def __init__(self, root, img_size=128):\n        self.samples = []\n        self.domain_names = [\"trainA\", \"trainB\", \"trainC\"]\n        for d_idx, folder in enumerate(self.domain_names):\n            path = os.path.join(root, folder)\n            if not os.path.isdir(path): continue\n            for person in os.listdir(path):\n                person_path = os.path.join(path, person)\n                if not os.path.isdir(person_path): continue\n                for file in glob.glob(os.path.join(person_path, \"*.*\")):\n                    if file.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\")):\n                        self.samples.append((file, d_idx))\n        print(f\"Dataset chargÃ© : {len(self.samples)} images\")\n        print(f\"RÃ©partition des domaines: {Counter([d for _,d in self.samples])}\")\n\n        self.transform = transforms.Compose([\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5]*3, [0.5]*3)\n        ])\n\n    def __len__(self): return len(self.samples)\n    def __getitem__(self, idx):\n        path, domain = self.samples[idx]\n        try:\n            img = Image.open(path).convert(\"RGB\")\n        except:\n            img = Image.new(\"RGB\", (CFG[\"img_size\"], CFG[\"img_size\"]))\n        return self.transform(img), domain, path\n\n# ====================== MODELS ======================\ndef weights_init(m):\n    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n        nn.init.normal_(m.weight, 0.0, 0.02)\n        if hasattr(m, 'bias') and m.bias is not None:\n            nn.init.constant_(m.bias, 0)\n\nclass AdaIN(nn.Module):\n    def __init__(self, channels, style_dim):\n        super().__init__()\n        self.norm = nn.InstanceNorm2d(channels, affine=False)\n        self.style = nn.Linear(style_dim, channels * 2)\n    def forward(self, x, s):\n        style = self.style(s)\n        gamma, beta = style.chunk(2, 1)\n        gamma = gamma.unsqueeze(-1).unsqueeze(-1)\n        beta = beta.unsqueeze(-1).unsqueeze(-1)\n        return gamma * self.norm(x) + beta\n\nclass ResBlock(nn.Module):\n    def __init__(self, channels, style_dim):\n        super().__init__()\n        self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\n        self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\n        self.adain1 = AdaIN(channels, style_dim)\n        self.adain2 = AdaIN(channels, style_dim)\n    def forward(self, x, s):\n        h = self.conv1(x)\n        h = self.adain1(h, s)\n        h = F.relu(h, inplace=True)\n        h = self.conv2(h)\n        h = self.adain2(h, s)\n        return x + h\n\nclass Generator(nn.Module):\n    def __init__(self, style_dim=64, n_res=4):\n        super().__init__()\n        self.enc = nn.Sequential(\n            nn.Conv2d(3, 64, 7, 1, 3, bias=False), nn.ReLU(True),\n            nn.Conv2d(64, 128, 4, 2, 1, bias=False), nn.ReLU(True),\n            nn.Conv2d(128, 256, 4, 2, 1, bias=False), nn.ReLU(True),\n        )\n        self.res = nn.ModuleList([ResBlock(256, style_dim) for _ in range(n_res)])\n        self.dec = nn.Sequential(\n            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False), nn.ReLU(True),\n            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False), nn.ReLU(True),\n            nn.Conv2d(64, 3, 7, 1, 3), nn.Tanh()\n        )\n    def forward(self, x, s):\n        h = self.enc(x)\n        for block in self.res:\n            h = block(h, s)\n        return self.dec(h)\n\nclass MappingNetwork(nn.Module):\n    def __init__(self, latent_dim=16, style_dim=64, n_domains=3):\n        super().__init__()\n        self.n_domains = n_domains\n        self.shared = nn.Sequential(\n            nn.Linear(latent_dim, 256), nn.ReLU(True),\n            nn.Linear(256, 256), nn.ReLU(True),\n        )\n        self.branches = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(256, 256), nn.ReLU(True),\n                nn.Linear(256, style_dim)\n            ) for _ in range(n_domains)\n        ])\n    def forward(self, z, domain):\n        h = self.shared(z)\n        styles = [branch(h) for branch in self.branches]\n        styles = torch.stack(styles, dim=1)\n        batch_size = z.size(0)\n        return styles[torch.arange(batch_size), domain]\n\nclass StyleEncoder(nn.Module):\n    def __init__(self, style_dim=64, n_domains=3):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(3, 64, 7, 1, 3), nn.ReLU(True),\n            nn.Conv2d(64, 128, 4, 2, 1), nn.ReLU(True),\n            nn.Conv2d(128, 256, 4, 2, 1), nn.ReLU(True),\n            nn.AdaptiveAvgPool2d(1),\n        )\n        self.style = nn.Linear(256, style_dim)\n        self.classifier = nn.Linear(256, n_domains)\n    def forward(self, x):\n        h = self.net(x).view(x.size(0), -1)\n        style = self.style(h)\n        cls_output = self.classifier(h)\n        return style, cls_output\n\nclass Discriminator(nn.Module):\n    def __init__(self, n_domains=3):\n        super().__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(3, 64, 4, 2, 1), nn.LeakyReLU(0.2, True),\n            nn.Conv2d(64, 128, 4, 2, 1), nn.LeakyReLU(0.2, True),\n            nn.Conv2d(128, 256, 4, 2, 1), nn.LeakyReLU(0.2, True),\n            nn.Conv2d(256, 512, 4, 2, 1), nn.LeakyReLU(0.2, True),\n            nn.Conv2d(512, 1024, 4, 1, 0), nn.LeakyReLU(0.2, True),\n        )\n        self.src = nn.Conv2d(1024, 1, 3, 1, 1)\n        self.cls = nn.Conv2d(1024, n_domains, 3, 1, 1)\n    def forward(self, x):\n        h = self.main(x)\n        src = self.src(h).view(x.size(0), -1)\n        cls = self.cls(h).mean([2,3])\n        return src, cls\n\n# ====================== DATA & MODELS ======================\ndataset = AgeDataset(CFG[\"data_root\"])\ncounts = Counter([d for _,d in dataset.samples])\nweights = [1.0 / counts[d] for _,d in dataset.samples]\nsampler = WeightedRandomSampler(weights, len(weights), replacement=True)\nloader = DataLoader(dataset, batch_size=CFG[\"batch_size\"], sampler=sampler,\n                    num_workers=CFG[\"num_workers\"], drop_last=True,\n                    pin_memory=True, persistent_workers=True)\n\nG = Generator(CFG[\"style_dim\"], CFG[\"n_res\"]).to(device)\nF_map = MappingNetwork(CFG[\"latent_dim\"], CFG[\"style_dim\"], CFG[\"n_domains\"]).to(device)\nF_style = StyleEncoder(CFG[\"style_dim\"], CFG[\"n_domains\"]).to(device)\nD = Discriminator(CFG[\"n_domains\"]).to(device)\n\nG.apply(weights_init); F_map.apply(weights_init); F_style.apply(weights_init); D.apply(weights_init)\n\nopt_g = torch.optim.Adam(list(G.parameters()) + list(F_map.parameters()) + list(F_style.parameters()), \n                         lr=CFG[\"lr\"], betas=(0.5, 0.999))\nopt_d = torch.optim.Adam(D.parameters(), lr=CFG[\"lr\"], betas=(0.5, 0.999))\nscaler_g = GradScaler()\nscaler_d = GradScaler()\n\n# ====================== TRAINING LOOP ======================\npbar = tqdm(total=CFG[\"iters\"])\ndomain_names = [\"JEUNE\", \"ADULTE\", \"Ã‚GÃ‰\"]\n\nfor it in range(1, CFG[\"iters\"] + 1):\n    real, real_domain, _ = next(iter(loader))\n    real = real.to(device, non_blocking=True)\n    real_domain = real_domain.to(device, non_blocking=True)\n    b = real.size(0)\n\n    # --- Discriminator ---\n    opt_d.zero_grad()\n    with autocast('cuda'):\n        tgt_domain = torch.randint(0, CFG[\"n_domains\"], (b,), device=device)\n        z = torch.randn(b, CFG[\"latent_dim\"], device=device)\n        style = F_map(z, tgt_domain)\n        fake = G(real, style).detach()\n\n        real_src, real_cls = D(real)\n        fake_src, _ = D(fake)\n\n        loss_d_adv = F.softplus(-real_src).mean() + F.softplus(fake_src).mean()\n        loss_d_cls = F.cross_entropy(real_cls, real_domain)\n        loss_d = loss_d_adv + CFG[\"lambda_cls\"] * loss_d_cls\n\n    scaler_d.scale(loss_d).backward()\n    scaler_d.step(opt_d)\n    scaler_d.update()\n\n    # --- Generator ---\n    opt_g.zero_grad()\n    with autocast('cuda'):\n        tgt_domain = torch.randint(0, CFG[\"n_domains\"], (b,), device=device)\n        z = torch.randn(b, CFG[\"latent_dim\"], device=device)\n        style = F_map(z, tgt_domain)\n        fake = G(real, style)\n        fake_src, fake_cls = D(fake)\n\n        loss_g_adv = -fake_src.mean()\n        loss_g_cls = F.cross_entropy(fake_cls, tgt_domain)\n\n        style_orig, _ = F_style(real)\n        rec = G(fake, style_orig)\n        loss_cyc = F.l1_loss(rec, real)\n\n        z2 = torch.randn(b, CFG[\"latent_dim\"], device=device)\n        style2 = F_map(z2, tgt_domain)\n        fake2 = G(real, style2)\n        loss_sty = -F.l1_loss(fake, fake2) * CFG[\"lambda_sty\"]\n\n        style_reg = F.l1_loss(style, torch.zeros_like(style)) * CFG[\"lambda_reg\"]\n\n        loss_g = (loss_g_adv + CFG[\"lambda_cls\"] * loss_g_cls + \n                 CFG[\"lambda_cyc\"] * loss_cyc + loss_sty + style_reg)\n\n    scaler_g.scale(loss_g).backward()\n    scaler_g.step(opt_g)\n    scaler_g.update()\n\n    # --- SAMPLING DIAGNOSTIQUE ---\n    if it % CFG[\"sample_every\"] == 0:\n        G.eval(); F_map.eval(); F_style.eval()\n        with torch.no_grad():\n            ref_images, ref_domains_list = [], []\n            for domain in range(CFG[\"n_domains\"]):\n                domain_mask = (real_domain == domain)\n                if domain_mask.sum() > 0:\n                    idx = torch.where(domain_mask)[0][0]\n                    ref_images.append(real[idx])\n                    ref_domains_list.append(domain)\n            \n            if len(ref_images) >= 2:\n                current_ref = torch.stack(ref_images[:min(4, len(ref_images))])\n                ref_domains = torch.tensor(ref_domains_list[:current_ref.size(0)], device=device)\n                \n                outs = [current_ref.cpu()]\n                \n                for target_domain in range(CFG[\"n_domains\"]):\n                    tgt_domains = torch.full((current_ref.size(0),), target_domain, device=device)\n                    z = torch.randn(current_ref.size(0), CFG[\"latent_dim\"], device=device)\n                    s = F_map(z, tgt_domains)\n                    gen = G(current_ref, s)\n                    # Appliquer cheveux gris et nettetÃ©\n                    for i in range(gen.size(0)):\n                        gen[i] = add_gray_hair(gen[i], tgt_domains[i], CFG[\"gray_hair_intensity\"])\n                        gen[i] = sharpen(gen[i], CFG[\"sharpen_alpha\"])\n                    outs.append(gen.cpu())\n\n                grid = torch.cat(outs, dim=0)\n                save_image(grid * 0.5 + 0.5, \n                          f\"{CFG['out_dir']}/samples/{it:06d}.jpg\", \n                          nrow=current_ref.size(0), padding=4, pad_value=1)\n                \n                ref_names = [domain_names[d] for d in ref_domains.cpu()]\n                print(f\"\\nğŸ­ Sample {it}: {ref_names} â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\")\n\n        G.train(); F_map.train(); F_style.train()\n\n       # --- Checkpoint (poids seulement pendant le training) ---\n    if it % CFG[\"save_every\"] == 0:\n        torch.save({\n            \"G\": G.state_dict(),\n            \"F_map\": F_map.state_dict(),\n            \"F_style\": F_style.state_dict(),\n            \"D\": D.state_dict(),\n            \"opt_g\": opt_g.state_dict(),\n            \"opt_d\": opt_d.state_dict(),\n            \"iter\": it,\n            \"CFG\": CFG\n        }, f\"{CFG['out_dir']}/checkpoints/checkpoint_{it}.pth\")\n\n    # --- Affichage progression ---\n    pbar.set_postfix({\n        \"D\": f\"{loss_d.item():.4f}\", \n        \"G\": f\"{loss_g.item():.4f}\",\n        \"Cyc\": f\"{loss_cyc.item():.4f}\",\n        \"Sty\": f\"{loss_sty.item():.4f}\"\n    })\n    pbar.update(1)\n\n# ====================== FIN TRAINING ======================\nprint(\"ğŸ‰ Training terminÃ© !\")\n\n# --- Sauvegarde MODELE COMPLET (architecture + poids) ---\ntorch.save({\n    \"G\": G,\n    \"F_map\": F_map, \n    \"F_style\": F_style,\n    \"D\": D,\n    \"CFG\": CFG\n}, f\"{CFG['out_dir']}/final_model_complete_40k.pth\")\n\nprint(\"âœ… ModÃ¨le complet sauvegardÃ© : final_model_complete_40k.pth\")\n\n# ====================== CrÃ©er ZIP pour tÃ©lÃ©chargement facile (Kaggle) ======================\nimport shutil\n\nshutil.make_archive(\n    base_name=f\"{CFG['out_dir']}/starganv2_complete_model\",\n    format=\"zip\",\n    root_dir=CFG[\"out_dir\"],\n    base_dir=\".\"\n)\n\nprint(\"ğŸ“¦ Archive prÃªte : starganv2_complete_model.zip\")\n\n\n# ====================== TEST FINAL ======================\nprint(\"\\nğŸ§ª Test final des transformations...\")\nG.eval(); F_map.eval()\nwith torch.no_grad():\n    test_img = real[0:1]\n    source_domain = real_domain[0].item()\n    print(f\"Image source: {domain_names[source_domain]}\")\n    \n    for target_domain in range(CFG[\"n_domains\"]):\n        if target_domain != source_domain:\n            z = torch.randn(1, CFG[\"latent_dim\"], device=device)\n            style = F_map(z, torch.tensor([target_domain], device=device))\n            result = G(test_img, style)\n            result = add_gray_hair(result[0], torch.tensor(target_domain), CFG[\"gray_hair_intensity\"]).unsqueeze(0)\n            result = sharpen(result[0], CFG[\"sharpen_alpha\"]).unsqueeze(0)\n            save_image(result * 0.5 + 0.5, \n                      f\"{CFG['out_dir']}/test_{domain_names[source_domain]}_to_{domain_names[target_domain]}.jpg\")\n            print(f\"  â†’ {domain_names[target_domain]} : sauvegardÃ©\")\n\nprint(\"âœ… Tous les tests sont terminÃ©s !\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T11:02:12.384182Z","iopub.execute_input":"2025-12-09T11:02:12.384924Z"}},"outputs":[{"name":"stdout","text":"Dataset chargÃ© : 71191 images\nRÃ©partition des domaines: Counter({1: 51736, 2: 15862, 0: 3593})\n","output_type":"stream"},{"name":"stderr","text":"  1%|â–         | 500/40000 [04:59<10:12:19,  1.08it/s, D=2.3837, G=1.4906, Cyc=0.1212, Sty=-0.0033]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":"  2%|â–         | 1000/40000 [09:53<6:35:54,  1.64it/s, D=2.3019, G=1.1347, Cyc=0.0908, Sty=-0.0016]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 1000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":"  4%|â–         | 1500/40000 [14:47<6:26:53,  1.66it/s, D=2.2146, G=1.0736, Cyc=0.0898, Sty=-0.0015]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 1500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":"  5%|â–Œ         | 2000/40000 [19:40<6:27:12,  1.64it/s, D=2.1727, G=0.7558, Cyc=0.0819, Sty=-0.0016]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 2000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":"  6%|â–‹         | 2500/40000 [24:34<6:16:02,  1.66it/s, D=2.0078, G=0.9947, Cyc=0.0761, Sty=-0.0010]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 2500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":"  8%|â–Š         | 3000/40000 [29:27<6:12:26,  1.66it/s, D=2.0180, G=0.8502, Cyc=0.0825, Sty=-0.0008]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 3000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":"  9%|â–‰         | 3500/40000 [34:19<6:10:44,  1.64it/s, D=1.8788, G=0.8984, Cyc=0.0709, Sty=-0.0013]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 3500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 10%|â–ˆ         | 4000/40000 [39:12<6:00:38,  1.66it/s, D=1.9114, G=0.8430, Cyc=0.0737, Sty=-0.0006]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 4000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 11%|â–ˆâ–        | 4500/40000 [44:05<5:54:14,  1.67it/s, D=1.9190, G=0.7867, Cyc=0.0729, Sty=-0.0007]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 4500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 12%|â–ˆâ–        | 4999/40000 [48:57<5:41:34,  1.71it/s, D=1.8165, G=0.6528, Cyc=0.0637, Sty=-0.0004]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 5000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 14%|â–ˆâ–        | 5500/40000 [53:49<5:40:33,  1.69it/s, D=1.8504, G=0.6987, Cyc=0.0636, Sty=-0.0005]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 5500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 15%|â–ˆâ–Œ        | 6000/40000 [58:41<5:39:21,  1.67it/s, D=1.8420, G=0.7126, Cyc=0.0658, Sty=-0.0007]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 6000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 16%|â–ˆâ–‹        | 6500/40000 [1:03:33<5:33:56,  1.67it/s, D=1.7220, G=0.7632, Cyc=0.0570, Sty=-0.0006]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 6500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 18%|â–ˆâ–Š        | 7000/40000 [1:08:24<5:33:15,  1.65it/s, D=1.8409, G=0.5645, Cyc=0.0582, Sty=-0.0006]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 7000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 19%|â–ˆâ–‰        | 7500/40000 [1:13:15<5:19:15,  1.70it/s, D=1.8170, G=0.6463, Cyc=0.0595, Sty=-0.0004]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 7500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 20%|â–ˆâ–ˆ        | 8000/40000 [1:18:06<5:25:33,  1.64it/s, D=1.7218, G=0.5589, Cyc=0.0623, Sty=-0.0003]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 8000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 21%|â–ˆâ–ˆâ–       | 8500/40000 [1:22:58<5:13:58,  1.67it/s, D=1.8867, G=0.7998, Cyc=0.0587, Sty=-0.0003]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 8500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 22%|â–ˆâ–ˆâ–       | 9000/40000 [1:27:49<5:09:45,  1.67it/s, D=1.7982, G=0.6176, Cyc=0.0576, Sty=-0.0003]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 9000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 24%|â–ˆâ–ˆâ–       | 9500/40000 [1:32:41<5:06:55,  1.66it/s, D=1.7842, G=0.7990, Cyc=0.0591, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 9500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 25%|â–ˆâ–ˆâ–       | 9999/40000 [1:37:32<4:50:30,  1.72it/s, D=1.8964, G=0.5498, Cyc=0.0588, Sty=-0.0004]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 10000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 26%|â–ˆâ–ˆâ–‹       | 10500/40000 [1:42:26<4:55:56,  1.66it/s, D=1.6133, G=0.5962, Cyc=0.0538, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 10500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 28%|â–ˆâ–ˆâ–Š       | 11000/40000 [1:47:19<4:48:31,  1.68it/s, D=1.7188, G=0.7344, Cyc=0.0623, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 11000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 29%|â–ˆâ–ˆâ–‰       | 11500/40000 [1:52:11<4:46:06,  1.66it/s, D=1.9384, G=0.7103, Cyc=0.0593, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 11500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 30%|â–ˆâ–ˆâ–ˆ       | 12000/40000 [1:57:02<4:41:21,  1.66it/s, D=1.6312, G=0.6979, Cyc=0.0552, Sty=-0.0003]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 12000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 31%|â–ˆâ–ˆâ–ˆâ–      | 12500/40000 [2:01:54<4:35:35,  1.66it/s, D=1.5953, G=0.6992, Cyc=0.0535, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 12500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 32%|â–ˆâ–ˆâ–ˆâ–      | 13000/40000 [2:06:46<4:29:37,  1.67it/s, D=1.5089, G=0.6654, Cyc=0.0527, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 13000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 34%|â–ˆâ–ˆâ–ˆâ–      | 13500/40000 [2:11:38<4:26:59,  1.65it/s, D=1.8141, G=0.9538, Cyc=0.0566, Sty=-0.0001]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 13500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 14000/40000 [2:16:29<4:19:28,  1.67it/s, D=1.5329, G=0.5532, Cyc=0.0579, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 14000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 36%|â–ˆâ–ˆâ–ˆâ–‹      | 14500/40000 [2:21:22<4:13:52,  1.67it/s, D=1.6752, G=0.8536, Cyc=0.0668, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 14500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 37%|â–ˆâ–ˆâ–ˆâ–‹      | 14999/40000 [2:26:12<4:03:19,  1.71it/s, D=1.7425, G=1.1387, Cyc=0.0584, Sty=-0.0003]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 15000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 39%|â–ˆâ–ˆâ–ˆâ–‰      | 15500/40000 [2:31:03<4:04:03,  1.67it/s, D=1.5842, G=0.5636, Cyc=0.0561, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 15500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16000/40000 [2:35:52<3:58:44,  1.68it/s, D=1.6995, G=0.7247, Cyc=0.0591, Sty=-0.0003]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 16000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16500/40000 [2:40:42<3:55:16,  1.66it/s, D=1.5983, G=0.6982, Cyc=0.0551, Sty=-0.0001]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 16500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17000/40000 [2:45:31<3:50:38,  1.66it/s, D=1.5876, G=0.4131, Cyc=0.0503, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 17000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17500/40000 [2:50:20<3:45:29,  1.66it/s, D=1.6256, G=0.7775, Cyc=0.0521, Sty=-0.0003]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 17500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18000/40000 [2:55:10<3:39:47,  1.67it/s, D=1.5958, G=0.5994, Cyc=0.0606, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 18000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 18500/40000 [3:00:00<3:34:57,  1.67it/s, D=1.4981, G=0.8128, Cyc=0.0577, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 18500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19000/40000 [3:04:50<3:28:54,  1.68it/s, D=1.5946, G=0.7211, Cyc=0.0498, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 19000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 19500/40000 [3:09:40<3:24:08,  1.67it/s, D=1.4019, G=0.6844, Cyc=0.0582, Sty=-0.0003]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 19500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 19999/40000 [3:14:31<3:13:20,  1.72it/s, D=1.4270, G=0.8630, Cyc=0.0529, Sty=-0.0003]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 20000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20500/40000 [3:19:24<3:16:03,  1.66it/s, D=1.4220, G=0.6137, Cyc=0.0525, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 20500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21000/40000 [3:24:16<3:10:25,  1.66it/s, D=1.8497, G=0.7090, Cyc=0.0600, Sty=-0.0003]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 21000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21500/40000 [3:29:08<3:04:59,  1.67it/s, D=1.4902, G=0.8217, Cyc=0.0597, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 21500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 22000/40000 [3:34:00<2:59:12,  1.67it/s, D=1.7002, G=0.6588, Cyc=0.0576, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 22000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 22500/40000 [3:38:51<2:54:24,  1.67it/s, D=1.4495, G=0.7265, Cyc=0.0569, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 22500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 23000/40000 [3:43:45<2:52:41,  1.64it/s, D=1.7065, G=1.1632, Cyc=0.0588, Sty=-0.0001]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 23000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 23500/40000 [3:48:39<2:46:33,  1.65it/s, D=1.8062, G=1.0926, Cyc=0.0559, Sty=-0.0001]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 23500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 24000/40000 [3:53:32<2:40:02,  1.67it/s, D=1.5020, G=1.0008, Cyc=0.0565, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 24000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24500/40000 [3:58:25<2:36:30,  1.65it/s, D=1.4431, G=0.8293, Cyc=0.0609, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 24500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24999/40000 [4:03:18<2:27:31,  1.69it/s, D=1.7126, G=0.8220, Cyc=0.0585, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 25000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 25500/40000 [4:08:12<2:28:05,  1.63it/s, D=1.6516, G=1.1885, Cyc=0.0529, Sty=-0.0001]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 25500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 26000/40000 [4:13:04<2:19:44,  1.67it/s, D=1.4511, G=0.6701, Cyc=0.0553, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 26000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 26500/40000 [4:17:55<2:13:32,  1.68it/s, D=1.6807, G=1.3025, Cyc=0.0551, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 26500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 27000/40000 [4:22:48<2:11:23,  1.65it/s, D=1.4663, G=1.0153, Cyc=0.0559, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 27000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 27500/40000 [4:27:42<2:04:55,  1.67it/s, D=1.3871, G=0.9857, Cyc=0.0538, Sty=-0.0003]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 27500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 28000/40000 [4:32:35<2:00:29,  1.66it/s, D=1.5356, G=0.9874, Cyc=0.0567, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 28000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 28500/40000 [4:37:29<1:55:05,  1.67it/s, D=1.4140, G=0.9156, Cyc=0.0535, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 28500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 29000/40000 [4:42:20<1:49:00,  1.68it/s, D=1.4904, G=0.6939, Cyc=0.0521, Sty=-0.0003]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 29000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 29500/40000 [4:47:10<1:44:45,  1.67it/s, D=1.6250, G=0.8779, Cyc=0.0554, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 29500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 29999/40000 [4:52:00<1:36:05,  1.73it/s, D=1.4079, G=1.1236, Cyc=0.0564, Sty=-0.0003]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 30000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 30500/40000 [4:56:51<1:32:40,  1.71it/s, D=1.2383, G=0.7014, Cyc=0.0502, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 30500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 31000/40000 [5:01:40<1:29:04,  1.68it/s, D=1.3374, G=1.0875, Cyc=0.0560, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 31000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 31500/40000 [5:06:29<1:23:41,  1.69it/s, D=1.5624, G=0.8451, Cyc=0.0549, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 31500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 32000/40000 [5:11:19<1:19:27,  1.68it/s, D=1.4069, G=0.7709, Cyc=0.0597, Sty=-0.0003]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 32000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32500/40000 [5:16:07<1:14:07,  1.69it/s, D=1.3578, G=1.1642, Cyc=0.0526, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 32500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 33000/40000 [5:20:56<1:08:55,  1.69it/s, D=1.4237, G=0.9048, Cyc=0.0499, Sty=-0.0003]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 33000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 33500/40000 [5:25:45<1:04:20,  1.68it/s, D=1.3275, G=1.0082, Cyc=0.0521, Sty=-0.0003]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 33500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 34000/40000 [5:30:34<1:00:09,  1.66it/s, D=1.5498, G=0.9704, Cyc=0.0511, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 34000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 34500/40000 [5:35:22<55:12,  1.66it/s, D=1.5161, G=0.8506, Cyc=0.0557, Sty=-0.0002]  ","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 34500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 34999/40000 [5:40:11<48:20,  1.72it/s, D=1.1644, G=0.9929, Cyc=0.0516, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 35000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 35500/40000 [5:45:01<45:24,  1.65it/s, D=1.3046, G=0.8293, Cyc=0.0539, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 35500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 36000/40000 [5:49:50<39:38,  1.68it/s, D=1.7435, G=1.2786, Cyc=0.0519, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 36000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36500/40000 [5:54:38<34:55,  1.67it/s, D=1.5061, G=0.8712, Cyc=0.0536, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 36500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37000/40000 [5:59:27<29:40,  1.69it/s, D=1.3998, G=1.1033, Cyc=0.0506, Sty=-0.0001]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 37000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37500/40000 [6:04:15<24:21,  1.71it/s, D=1.3058, G=1.3437, Cyc=0.0536, Sty=-0.0002] ","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 37500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 38000/40000 [6:09:04<19:36,  1.70it/s, D=1.3165, G=0.8999, Cyc=0.0533, Sty=-0.0001]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 38000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38500/40000 [6:13:52<14:50,  1.68it/s, D=1.5835, G=1.4969, Cyc=0.0594, Sty=-0.0001]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 38500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 39000/40000 [6:18:42<10:01,  1.66it/s, D=1.4718, G=1.0517, Cyc=0.0582, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 39000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":" 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 39500/40000 [6:23:30<04:57,  1.68it/s, D=1.3495, G=0.7775, Cyc=0.0503, Sty=-0.0001]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 39500: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 39999/40000 [6:28:18<00:00,  1.73it/s, D=1.3172, G=0.9196, Cyc=0.0565, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"\nğŸ­ Sample 40000: ['JEUNE', 'ADULTE', 'Ã‚GÃ‰'] â†’ [JEUNE, ADULTE, Ã‚GÃ‰]\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40000/40000 [6:28:19<00:00,  1.45it/s, D=1.4058, G=1.2087, Cyc=0.0530, Sty=-0.0002]","output_type":"stream"},{"name":"stdout","text":"ğŸ‰ Training terminÃ© !\nâœ… ModÃ¨le complet sauvegardÃ© : final_model_complete_40k.pth\n","output_type":"stream"}],"execution_count":null}]}